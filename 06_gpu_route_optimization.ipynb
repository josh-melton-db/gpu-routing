{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e862297-c644-465b-8b3b-d2abe1277225",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install CuOpt"
    }
   },
   "outputs": [],
   "source": [
    "%pip install folium\n",
    "%pip install -q --extra-index-url=https://pypi.nvidia.com cuopt-server-cu12 cuopt-sh-client cuopt-cu12==25.8.*\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835a1815-de23-48b9-824b-0a17ca51ea4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Configs"
    }
   },
   "outputs": [],
   "source": [
    "NUM_SHIPMENTS = 20_000\n",
    "NUM_ROUTES = int(round(NUM_SHIPMENTS / 250 )) # total trucks available\n",
    "MAX_EV = 4000 # max capacity\n",
    "MAX_VAN = 8000\n",
    "DEPOT_LAT, DEPOT_LON = 39.7685, -86.1580 # start and end point for each route\n",
    "SOLVER_MINUTES = 10\n",
    "\n",
    "\n",
    "catalog = \"default\"\n",
    "schema = f\"routing\"\n",
    "shipments_table = f\"{catalog}.{schema}.raw_shipments_{NUM_SHIPMENTS}\"\n",
    "mapping_table = f\"{catalog}.{schema}.shipment_ids_map_{NUM_SHIPMENTS}\"\n",
    "clustered_table = f\"{catalog}.{schema}.shipment_clusters_gpu_{NUM_SHIPMENTS}\"\n",
    "distances_table = f\"{catalog}.{schema}.distances_by_route_gpu_{NUM_SHIPMENTS}\"\n",
    "routing_table = f\"{catalog}.{schema}.routing_unified_by_cluster_gpu_{NUM_SHIPMENTS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa9a5c1-06dd-469f-b92f-3985dd31486f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check GPU"
    }
   },
   "outputs": [],
   "source": [
    "%sh nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd04618-1a88-4512-884a-3e00ec5c844c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuopt import routing, distance_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3abf097b-4a69-4786-bcd9-57ede27591f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple Example"
    }
   },
   "outputs": [],
   "source": [
    "cost = cudf.DataFrame([[0,3,1,2],[3,0,1,2],[2,3,0,2],[2,3,1,0]], dtype='float32')\n",
    "n_locations = cost.shape[0]\n",
    "n_vehicles = 2\n",
    "n_orders = 3  # one order per task node\n",
    "\n",
    "dm = routing.DataModel(n_locations, n_vehicles, n_orders)\n",
    "dm.add_cost_matrix(cost)\n",
    "dm.add_transit_time_matrix(cost.copy(deep=True))  # separate if times differ\n",
    "\n",
    "ss = routing.SolverSettings()\n",
    "ss.set_verbose_mode(True)\n",
    "# ss.set_time_limit(5)\n",
    "sol = routing.Solve(dm, ss)\n",
    "\n",
    "print(sol.get_route())      # pandas-like table\n",
    "sol.display_routes()        # pretty print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "29af4d8a-7b91-495a-aac1-413195065d34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple Waypoint Example"
    }
   },
   "outputs": [],
   "source": [
    "# Hello World: cuOpt with CSR Waypoint Graph (no dense NÃ—N)\n",
    "\n",
    "import numpy as np\n",
    "import cudf\n",
    "from cuopt import distance_engine, routing\n",
    "\n",
    "\n",
    "base = np.array([\n",
    "    [0,3,1,2],\n",
    "    [3,0,1,2],\n",
    "    [2,3,0,2],\n",
    "    [2,3,1,0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "V = base.shape[0]\n",
    "# Build CSR: for each src i, edges to all j != i\n",
    "indices = []\n",
    "weights = []\n",
    "offsets = [0]\n",
    "for i in range(V):\n",
    "    for j in range(V):\n",
    "        if i == j:\n",
    "            continue\n",
    "        indices.append(j)\n",
    "        weights.append(float(base[i, j]))\n",
    "    offsets.append(len(indices))\n",
    "\n",
    "indices = np.asarray(indices, dtype=np.int32)        # size E = 12\n",
    "weights = np.asarray(weights, dtype=np.float32)      # size E\n",
    "offsets = np.asarray(offsets, dtype=np.int32)        # size V+1\n",
    "\n",
    "# ----- 2) Build Waypoint graph and compute compact matrix for targets -----\n",
    "wg = distance_engine.WaypointMatrix(offsets, indices, weights)\n",
    "targets = np.arange(V, dtype=np.int32)               # use all 4 nodes; 0 will be the depot\n",
    "cost = wg.compute_cost_matrix(targets)               # cudf.DataFrame (4x4)\n",
    "\n",
    "# ----- 3) Build a minimal routing model and solve -----\n",
    "n_locations = len(targets)\n",
    "n_vehicles  = 2\n",
    "n_orders    = 3\n",
    "\n",
    "dm = routing.DataModel(n_locations, n_vehicles, n_orders)\n",
    "\n",
    "# vehicles start/end at depot (index 0 in our target set)\n",
    "dm.set_vehicle_locations(\n",
    "    cudf.Series([0]*n_vehicles),   # starts\n",
    "    cudf.Series([0]*n_vehicles)    # ends\n",
    ")\n",
    "\n",
    "# 3 orders at locations 1,2,3  (indices within the compact matrix)\n",
    "dm.set_order_locations(cudf.Series([1,2,3]))\n",
    "\n",
    "# Primary matrices\n",
    "dm.add_cost_matrix(cost)\n",
    "\n",
    "# Optional: require both vehicles to be used (just to see multiple routes)\n",
    "dm.set_min_vehicles(n_vehicles)\n",
    "\n",
    "ss = routing.SolverSettings()\n",
    "ss.set_verbose_mode(True)\n",
    "# ss.set_time_limit(5)  # optional\n",
    "\n",
    "sol = routing.Solve(dm, ss)\n",
    "print(sol.get_route())   # pandas-like table\n",
    "sol.display_routes()     # pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d0bdff-1910-4114-8991-de3a74889d04",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare Data"
    }
   },
   "outputs": [],
   "source": [
    "DEPOT_ID = 0\n",
    "\n",
    "distances_df = (\n",
    "  spark.read.table(distances_table)\n",
    "  .select(\"global_idx_source\", \"global_idx_dest\", \"duration_seconds\")\n",
    ")\n",
    "\n",
    "rev_to_depot = (\n",
    "    distances_df\n",
    "      .where(F.col(\"global_idx_dest\") == DEPOT_ID)\n",
    "      .select(\n",
    "          F.lit(DEPOT_ID).alias(\"global_idx_source\"),\n",
    "          F.col(\"global_idx_source\").alias(\"global_idx_dest\"),\n",
    "          F.col(\"duration_seconds\")\n",
    "      )\n",
    ")\n",
    "\n",
    "distances_df = (\n",
    "    distances_df\n",
    "    .unionByName(rev_to_depot)\n",
    "    # .orderBy(\"global_idx_source\", \"global_idx_dest\")\n",
    ")  \n",
    "display(distances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0604277-cedd-4c3f-bbe5-33a3d63cd3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Questions**\n",
    "- Works for 20k packages on a 10 min solver time limit, but do longer solves take more memory?\n",
    "- Do we need both cost and time matrices?\n",
    "- Will it help memory constraints to round durations to ints?\n",
    "- (tentatively done) How do we set a max duration per truck?\n",
    "\n",
    "**Notes**\n",
    "- OOMs at 40k packages\n",
    "- TODO: run trials on CPU vs GPU\n",
    "- not in cuopt yet, but cupy supports sparse matrices https://docs.cupy.dev/en/v9.6.0/reference/sparse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f5fbbb-3821-4f2a-96fd-15b17aebe95e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solve!"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Pull edges and normalize\n",
    "# ---------------------------\n",
    "pdf = (\n",
    "    distances_df\n",
    "      .select(\"global_idx_source\",\"global_idx_dest\",\"duration_seconds\")\n",
    "      .toPandas()\n",
    ")\n",
    "\n",
    "# Build stable 0..n-1 index space for ALL nodes seen in src or dest\n",
    "all_nodes = pd.Index(pd.unique(pd.concat([pdf[\"global_idx_source\"],\n",
    "                                          pdf[\"global_idx_dest\"]], ignore_index=True)))\n",
    "node2pos = {int(g): i for i, g in enumerate(all_nodes)}\n",
    "n = len(all_nodes)\n",
    "\n",
    "pdf[\"src_idx\"] = pdf[\"global_idx_source\"].map(node2pos).astype(np.int32)\n",
    "pdf[\"dst_idx\"] = pdf[\"global_idx_dest\"].map(node2pos).astype(np.int32)\n",
    "pdf[\"cost\"]    = pdf[\"duration_seconds\"].astype(np.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Build CSR (offsets/indices/weights)\n",
    "# ---------------------------\n",
    "pdf = pdf.sort_values([\"src_idx\",\"dst_idx\"], kind=\"mergesort\")\n",
    "\n",
    "indices = pdf[\"dst_idx\"].to_numpy(dtype=np.int32)       # E-length array of neighbor dsts\n",
    "weights = pdf[\"cost\"].to_numpy(dtype=np.float32)        # E-length array of edge costs\n",
    "\n",
    "# counts per src over the FULL 0..n-1 range (nodes with 0 out-edges still get an offset)\n",
    "counts = (\n",
    "    pdf.groupby(\"src_idx\").size()\n",
    "       .reindex(range(n), fill_value=0)\n",
    "       .to_numpy(dtype=np.int32)\n",
    ")\n",
    "\n",
    "# offsets[v]..offsets[v+1]-1 slice into `indices`/`weights`\n",
    "offsets = np.concatenate([[0], np.cumsum(counts, dtype=np.int64)]).astype(np.int32)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Waypoint graph + compact matrix for selected targets\n",
    "# ---------------------------\n",
    "wg = distance_engine.WaypointMatrix(offsets, indices, weights)\n",
    "order_globals = [int(x) for x in all_nodes if int(x) != DEPOT_ID]\n",
    "\n",
    "# Targets are the nodes we want in the compact matrix: [depot] + orders\n",
    "targets = np.array(\n",
    "    [node2pos[DEPOT_ID]] + [node2pos[g] for g in order_globals],\n",
    "    dtype=np.int32\n",
    ")\n",
    "\n",
    "cost = wg.compute_cost_matrix(targets)   # cudf.DataFrame (len(targets) x len(targets))\n",
    "time = cost.copy(deep=True)              # use cost as time for now\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Routing model and solve\n",
    "# ---------------------------\n",
    "MAX_ROUTE_SECONDS = 60*60*9 # 9 hours max route time\n",
    "n_locations = len(targets)\n",
    "n_orders    = len(order_globals)\n",
    "\n",
    "dm = routing.DataModel(n_locations, NUM_ROUTES, n_orders)\n",
    "dm.set_vehicle_locations(cudf.Series([0]*NUM_ROUTES), cudf.Series([0]*NUM_ROUTES))\n",
    "dm.set_order_locations(cudf.Series(np.arange(1, n_locations, dtype=np.int32)))\n",
    "dm.set_vehicle_max_times(cudf.Series(np.full(NUM_ROUTES, MAX_ROUTE_SECONDS, dtype=np.float32)))\n",
    "\n",
    "# Primary matrices\n",
    "dm.add_cost_matrix(cost)\n",
    "dm.add_transit_time_matrix(time)\n",
    "dm.set_min_vehicles(NUM_ROUTES)\n",
    "\n",
    "ss = routing.SolverSettings()\n",
    "ss.set_verbose_mode(True)\n",
    "ss.set_time_limit(SOLVER_MINUTES * 60)  # time limit in seconds\n",
    "\n",
    "sol = routing.Solve(dm, ss)\n",
    "sol.display_routes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0ddf94-9d17-4a27-942a-e900e4b75d55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Results"
    }
   },
   "outputs": [],
   "source": [
    "route_pdf = sol.get_route().to_pandas()\n",
    "optimized_routes_df = spark.createDataFrame(route_pdf)\n",
    "optimized_routes_df.write.mode(\"overwrite\").saveAsTable(routing_table)\n",
    "routing_df = spark.read.table(routing_table)\n",
    "display(routing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "896ee86d-a0f9-4901-aaaf-0d03a15c797e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760035542950}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"cm91dGluZ19kZiA9IHNwYXJrLnJlYWQudGFibGUoImRlZmF1bHQucm91dGluZy5yb3V0aW5nX3VuaWZpZWRfYnlfY2x1c3Rlcl9ncHVfMjAwMDAiKQpkaXNwbGF5KHJvdXRpbmdfZGYp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView05f829a\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView05f829a\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView05f829a\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView05f829a) SELECT `truck_id`,MAX(`arrival_stamp`) `column_93d62044233` FROM q GROUP BY `truck_id`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView05f829a\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "truck_id",
             "id": "column_93d62044231"
            },
            "y": [
             {
              "column": "arrival_stamp",
              "id": "column_93d62044233",
              "transform": "MAX"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_93d62044233": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1760030793172,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "table",
         80
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "dfdf00a2-ce4a-47bf-aa01-6d820b598f01",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 16.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1760030792467,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "filters": [],
         "groups": [
          {
           "column": "truck_id",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "truck_id",
           "type": "column"
          },
          {
           "alias": "column_93d62044233",
           "args": [
            {
             "column": "arrival_stamp",
             "type": "column"
            }
           ],
           "function": "MAX",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 1760030792407,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "routing_df = spark.read.table(\"default.routing.routing_unified_by_cluster_gpu_20000\")\n",
    "display(routing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b96b9b82-2cc7-414b-a4b6-343ae89205c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "NUM_SHIPMENTS = 20_000\n",
    "catalog = \"default\"\n",
    "schema = f\"routing\"\n",
    "shipments_table = f\"{catalog}.{schema}.raw_shipments_{NUM_SHIPMENTS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "89735bc0-2b3d-45c4-9955-21c894b436d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.plotter import plot_route_folium\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Recreate the order_globals mapping from cell 9\n",
    "# This maps compact matrix positions to actual global indices\n",
    "DEPOT_ID = 0\n",
    "\n",
    "# Get all global indices except depot (same logic as cell 9)\n",
    "distances_df = spark.read.table(\"default.routing.distances_by_route_gpu_20000\")\n",
    "all_nodes_df = (\n",
    "    distances_df\n",
    "    .select(\"global_idx_source\").distinct()\n",
    "    .union(distances_df.select(\"global_idx_dest\").distinct())\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "order_globals_df = (\n",
    "    all_nodes_df\n",
    "    .where(F.col(\"global_idx_source\") != DEPOT_ID)\n",
    "    .orderBy(\"global_idx_source\")\n",
    "    .withColumn(\"compact_location\", F.row_number().over(Window.orderBy(\"global_idx_source\")))\n",
    ")\n",
    "\n",
    "print(\"Order globals mapping (first 10):\")\n",
    "order_globals_df.show(10)\n",
    "\n",
    "# Get a sample route\n",
    "route = routing_df.select(\"route\").limit(1).collect()[0][0]\n",
    "print(f\"\\nAnalyzing route: {route}\")\n",
    "\n",
    "# Add route_index column\n",
    "window_spec = Window.partitionBy(\"route\").orderBy(\"arrival_stamp\")\n",
    "single_route_df = (\n",
    "    routing_df\n",
    "    .where(f\"route={route}\")\n",
    "    .withColumn(\"route_index\", F.row_number().over(window_spec) - 1)\n",
    ")\n",
    "\n",
    "print(\"Route data:\")\n",
    "single_route_df.select(\"route\", \"location\", \"arrival_stamp\", \"route_index\", \"type\").show()\n",
    "\n",
    "# Handle depot (location=0)\n",
    "depot_df = (\n",
    "    single_route_df\n",
    "    .where(F.col(\"location\") == 0)\n",
    "    .withColumn(\"latitude\", F.lit(39.7685))\n",
    "    .withColumn(\"longitude\", F.lit(-86.1580))\n",
    "    .withColumn(\"origin_id\", F.lit(\"DEPOT\"))\n",
    ")\n",
    "\n",
    "# For location>0, map through order_globals to get actual global_idx\n",
    "mapping_df = spark.read.table(\"default.routing.shipment_ids_map_20000\")\n",
    "\n",
    "shipment_df = (\n",
    "    single_route_df\n",
    "    .where(F.col(\"location\") > 0)\n",
    "    .join(\n",
    "        order_globals_df,\n",
    "        single_route_df[\"location\"] == order_globals_df[\"compact_location\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        mapping_df,\n",
    "        order_globals_df[\"global_idx_source\"] == mapping_df[\"global_idx\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        single_route_df[\"route\"],\n",
    "        single_route_df[\"arrival_stamp\"],\n",
    "        single_route_df[\"truck_id\"],\n",
    "        single_route_df[\"location\"],\n",
    "        single_route_df[\"type\"],\n",
    "        single_route_df[\"route_index\"],\n",
    "        mapping_df[\"latitude\"],\n",
    "        mapping_df[\"longitude\"],\n",
    "        mapping_df[\"package_id\"].alias(\"origin_id\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\nShipment data after proper mapping: {shipment_df.count()} records\")\n",
    "if shipment_df.count() > 0:\n",
    "    print(\"Sample shipment data:\")\n",
    "    shipment_df.select(\"location\", \"latitude\", \"longitude\", \"origin_id\").show(5)\n",
    "\n",
    "# Combine depot and shipment data\n",
    "combined_route_df = depot_df.unionByName(shipment_df)\n",
    "single_route_pdf = combined_route_df.orderBy(\"route_index\").toPandas()\n",
    "\n",
    "print(f\"\\nFinal route visualization data: {len(single_route_pdf)} stops\")\n",
    "if len(single_route_pdf) > 0:\n",
    "    print(\"All stops:\")\n",
    "    print(single_route_pdf[['route_index', 'location', 'latitude', 'longitude', 'origin_id']])\n",
    "    \n",
    "    # Plot the route\n",
    "    m = plot_route_folium(single_route_pdf)\n",
    "    display(m)\n",
    "else:\n",
    "    print(\"No route data to visualize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2b296b-90e3-405e-bf99-3514c73ebbc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the [Databricks License](https://databricks.com/db-license-source).  All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                | description                                                                                      | license      | source                                                    |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------|--------------|-----------------------------------------------------------|\n",
    "| OSRM Backend Server    | High performance routing engine written in C++14 designed to run on OpenStreetMap data           | BSD 2-Clause \"Simplified\" License | https://github.com/Project-OSRM/osrm-backend              |\n",
    "| osmnx                  | Download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap in Python | MIT License  | https://github.com/gboeing/osmnx                          |\n",
    "| ortools                | Operations research tools developed at Google for combinatorial optimization                     | Apache License 2.0 | https://github.com/google/or-tools                        |\n",
    "| folium                 | Visualize data in Python on interactive Leaflet.js maps                                          | MIT License  | https://github.com/python-visualization/folium            |\n",
    "| dash                   | Python framework for building analytical web applications and dashboards; built on Flask, React, and Plotly.js | MIT License  | https://github.com/plotly/dash                            |\n",
    "| branca                 | Library for generating complex HTML+JS pages in Python; provides non-map-specific features for folium | MIT License  | https://github.com/python-visualization/branca            |\n",
    "| plotly                 | Open-source Python library for creating interactive, publication-quality charts and graphs        | MIT License  | https://github.com/plotly/plotly.py                       |\n",
    "ray |\tFlexible, high-performance distributed execution framework for scaling Python workflows |\tApache2.0 |\thttps://github.com/ray-project/ray"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8287725918246293,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "06_gpu_route_optimization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
